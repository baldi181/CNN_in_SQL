\chapter{Einleitung}
\label{kap:intro}
Assistenzsysteme stellen Informationen und Hilfestellungen bei bestimmten Produkten bereit, um deren Bedienung zu erleichtern. Im Allgemeinen dienen sie der Verbesserung beziehungsweise Erleichterung verschiedenster Situationen aus dem modernen Alltag.
Heutzutage spielen Assistenzsysteme in fast allen Bereichen der Forschung und Industrie eine große Rolle und unterstützen Menschen bei ihren Tätigkeiten \cite{winner2014handbook, kurihata2005rainy, omerdic2011design}. Um sinnvolle Hilfestellungen zu geben, ist die korrekte Erfassung der aktuellen Situation und
der wahrscheinlichen Intentionen der zu assistierenden Benutzer notwendig. 
In diesem Zuge werden solche Systeme meist intelligent genannt, denn sie sind in der Lage, durch die Verarbeitung von Sensordaten bestimmte Szenarien und Aktivitäten zu erkennen, um so die optimalen Assistenzfunktionen zur Verfügung zu stellen. Darüber hinaus können diese Modelle genutzt werden, um Vorhersagen über zukünftige Situationen zu treffen.
Sensoren, z.B. in Smartphones, Autos oder allgemeinen Multimediasystemen, liefern im Kontext des \textit{Internet of Things}\cite{xia2012internet, wortmann2015internet}meist große Datenmengen, die geschickt verarbeitet werden müssen. Um Muster in diesen immer größer werdenden Datenmegen zu erkennen, können Verfahren und Algorithmen des \textit{Maschinellen Lernens}\cite{Goodfellow-et-al-2016}, kurz ML, genutzt werden. Die Analyse von Sensordaten für solche Assistenzsysteme ist daher Forschungsgegenstand in den Bereichen der Big Data Analytics\cite{d2019big} und Künstlichen Intelligenz. 

Das PArADISE-Projekt\cite{paradise} des Lehrstuhls für Datenbank- und Informationssysteme der Universität Rostock beschäftigt sich mit dem Designprozess von Assistenzsystemen mit dem Ziel, die Ersteller von assistiven Systemen zu unterstützen. In der Entwicklungsphase versuchen Datenwissenschaftler, Benutzeraktivitäten zu erkennen und vorherzusagen, indem sie Sensordaten von einer kleinen Anzahl von Testpersonen über einen kurzen Zeitraum sammeln. Diese Daten werden dann durch Expertenwissen mit Aktivitätsinformationen versehen, um anschließend Aktivitätsmodelle mit Hilfe von Maschinellen-Lern-Algorithmen zu lernen. Hier werden Verfahren des überwachten maschinellen Lernens genutzt, welche sich typischerweise in zwei Phasen einteilen lassen. In einer Trainingsphase werden mithilfe von annotierten Trainingsdaten Modelle zur Erkennung von Mustern abgeleitet. In der späteren Erkennungsphase wird dann das
trainierte Modell genutzt, um erkannte Muster in einer Zieldatenmenge effizient ableiten
zu können. 
\section*{Motivation}
Eines der schwierigsten Probleme stellt das Verwalten der riesigen Datenmengen, besonders in der Trainingsphase maschineller Lernverfahren, dar. Um in der Implementierung Performance-Probleme zu lösen, können die verwendete Hardware verbessert, der Code optimiert oder Parallelisierungstechniken wie das berühmte \textit{MapReduce}-Verfahren\cite{DBLP:journals/cn/BrinP98} benutzt werden. Ein andere Möglichkeit besteht in der Verwendung einer transparenten Datenbankunterstützung\cite{DBLP:conf/gvd/MartenH15} für Big Data Analytics. In diesem Kontext werden Techniken aus relationalen Datenbanksystemen eingebunden, um ML-Tools sinnvoll zu unterstützen.
 % was sich als spannendes Forschungsgebiet der Informationssysteme herausstellt \cite{abiteboul2018research}. 
Der Schlüssel liegt dabei in der Transformation von Maschinellen-Lern-Algorithmen in SQL-Datenbanksysteme. Gelingt diese Übersetzung, so können verschiedenste Resultate der relationalen Datenbankforschung vorteilhaft genutzt werden:
\begin{itemize}
    \item Techniken der parallelen Datenbanksysteme ermöglichen es, SQL-Anfragen auf Rechnercluster zu verteilen, um so die Performance von ML-Algorithmen zu verbessern. Besonders interessant ist hier die Umsetzung von Operationen der linearen Algebra, unter anderem motiviert durch Test-of Time-Award-Winner Dan Suciu \cite{interviewsuciu}. Die Transformation von Matrixoperationen in parallele relationale Datenbanksysteme bei dichtbesetzten bzw. dünnbesetzten Problemen wird beispielsweise in Marten et.al.\cite{martensparse} beleuchtet.
    \item Konzepte wie \textit{Query Decomposition} \cite{chirkova2011materialized} und \textit{Answering Queries using Views} \cite{ afrati2019answering, levy1999answering} werden genutzt, um bestimmte Auswertungen von Daten näher an den Sensoren durchzuführen und damit \textit{Privacy}-Aspekte \cite{agrawal2000privacy} der Benutzer zu berücksichtigen.
    \item \textit{Data Provenance} \cite{heuer2015metis, bruder2017konzepte} kann genutzt werden, um herauszufinden, welche Daten für die Detektierung bzw. Vorhersage von Aktivitäten gebraucht werden. 
    So wird unter anderem festgestellt, welche der vielen eingesetzten Sensoren für das Modell essentiell beziehungsweise uninteressant sind.
\end{itemize}

In der Arbeit von Marten et.al.\cite{marten2017machine} wird am Beispiel eines Meeting-Szenarios der Einsatz von Hidden-Markov-Modelle untersucht. Es wird erläutert, wie die Erkennungsphase eines zuvor trainierten Modells datenbankgestützt in parallelen Datenbanksystemen realisiert werden kann.   
Als Ergebnis wird festgehalten, dass die datenbankgestützte Umsetzung von ML-Algorithmen, hier speziell bei Hidden-Markov-Modellen, in bestimmten Szenarien gute Skalierungseigenschaften hinsichtlich der Datenmenge besitzt und verschiedenste Resultate der relationalen Datenbankforschung vorteilhaft genutzt werden können. Dies motiviert die Analyse weiterer ML-Verfahren und deren Transformation in SQL. 

\section*{Problemstellung}
\label{abs:problemstellung}
Diese Arbeit beschäftigt sich mit \textit{Convolutional Neural Networks}, kurz CNN, als weitverbreitete ML-Methode und deren Realisierung in SQL-Datenbanksystemen unter Zuhilfenahme von Werkzeugen der linearen Algebra. Die fundamentale mathematische Operation innerhalb von CNN stellt die Faltung $ f \ast g$ zweier Funktionen bzw. Signale $f$ und $g$ dar. 
Es liegt daher nahe, die Faltungsoperation von zeitdiskreten Signalen, in dieser Arbeit einfach Matrizen, datenbankgestützt umzusetzen.

\begin{problem}
    \label{prob:conv_in_sql}
Sind zwei Matrizen $X \in \RR^{m \times n}$ und $K \in \RR^{k \times k}$ gegeben, so ist die Matrixfaltung, siehe Definition \ref{def:matrix_faltung}, in eine Folge von SQL-Anweisungen zu übersetzen. Der Speicher- und Zeitaufwand dieser Anweisungen ist in Abhängigkeit von der Dimension von $X$ zu untersuchen.   
\end{problem}

CNN werden zur Lösung von Aufgaben der Computergrafik\cite{DBLP:conf/nips/KrizhevskySH12, DBLP:journals/pieee/LeCunBBH98,DBLP:conf/cvpr/CiresanMS12} erfolgreich genutzt.
Im Kontext der Mustererkennung soll die Klassifikation von digitalisierten Bildern als Anwendungsbeispiel durch ein vorher trainiertes CNN-Modell dienen. In dieser Arbeit werden dazu Grauwertbilder aus dem MNIST-Datensatz\cite{lecun1998gradient} genutzt. 

\begin{defi}
    \label{def:image}
    Eine Matrix $X \in [0,1]^{h \times b}$ heißt (Grauwert)-Bild mit der Höhe $h$ und Breite $b$. Mit $X_{i,j}$ wird der Grauwert des Pixels $p=(i,j)$ bezeichnet. In dieser Arbeit werden Matrizen als diskrete zweidimensionale Signale interpretiert.
\end{defi}
Der MNIST-Datensatz bietet 60.000 Trainingsbilder handgeschriebener Ziffern und 10.000 Testbilder, welche jeweils durch menschliches
Wissen annotiert sind. Dieser Datensatz kann als Benchmark zur Klassifikation von Grauwertbildern genutzt werden. Um dabei erfolgreich zu sein, muss ein CNN vorher mithilfe von Trainingsdaten angelernt werden. Hier wird Backpropagation, etabliert von Rumelhart et. al.\cite{MLPbook}, als Lernalgorithmus basierend auf dem Gradientenverfahren 
genutzt.
\begin{problem}
    \label{prop:train}
    Die Trainingsphase neuronaler Netze und speziell gefalteter neuronaler Netze gilt es konzeptionell darzustellen sowie am Beispiel des MNIST-Datensatzes für ein konkret gewähltes Modell möglichst performant umzusetzen. 
\end{problem}

Schließlich soll die datenbankgestützte Erkennungsphase in einem trainierten gefalteten neuronalen Netz implementiert werden.

\begin{problem}
    \label{prob:ffCCN}
    Angenommen, es wird ein trainiertes CNN als Modell zur Klassifikation von Grauwertbildern aus dem MNIST-Datensatz genutzt. Die Vorwärtsrechnung, vgl. Definition \ref{def_fw_cnn}, ist in eine Folge von SQL-Anfragen zu transformieren.
\end{problem}

\section*{Aufbau der Arbeit}
\label{abs:glied}
Im Kapitel \ref{kap:fund} wird zunächst das Maschinelle Lernen als mathematisches Problem eingeordnet und die konkrete Klassifikationsaufgabe im Kontext des überwachten Lernens definiert. Im Abschnitt \ref{abs:mathe_intro} wird der Begriff der linearen Trennbarkeit erläutert und ein erster Lernalgorithmus vorgestellt. Im folgenden Abschnitt \ref{abs:relation_intro} werden wichtige Grundbegriffe relationaler Datenbanksysteme erläutert und erklärt, wie Daten in Relationen repräsentiert und verarbeitet werden können. Hier gelingt die Darstellung von Objekten der linearen Algebra als Relationen und die damit verbundenen Basisoperationen, beispielsweise die Matrixvektormultiplikation, welche datenbankgestützt umgesetzt werden.

Im Kapitel \ref{kap:NN} werden Künstliche Neuronale Netze, kurz KNN, als Forschungsgegenstand der Informatik eingeführt und deren mathematische Grundlagen präzisiert. Im Hinblick auf CNN werden vorwärtsgerichtete neuronale Netze, kurz FFN, im Abschnitt \ref{MLP_abs} definiert und deren Training hinsichtlich der Klassifikationsaufgabe im Abschnitt \ref{abs:task_training} erläutert. 

Darauf aufbauend werden im Kapitel \ref{kap:CNN} gefaltete neuronale Netze als spezielle neuronale Netze eingeführt und deren Vorteile gegenüber FFN hinsichtlich der Bildklassifikation kurz erläutert. In den Abschnitten \ref{abs:conv_theorie} und \ref{abs:CNN_arch} werden die Faltungsoperation und die CNN-Architektur vorgestellt. Die Problemstellung \ref{prop:train} hinsichtlich des Trainings von gefalteten neuronalen Netzen wird im Abschnitt \ref{abs:CNN_train} behandelt.

Das Kapitel \ref{kap:CNN_in_SQL} beschäftigt sich mit der Transformation von gefalteten neuronalen Netzen und deren Operationen in SQL-Anweisungen. Im Abschnitt \ref{abs:conv_in_sql} werden Antworten zu Problem \ref{prob:conv_in_sql} geliefert und numerische Resultate vorgestellt. Im Abschnitt \ref{abs_CNN_in_SQL} wird hinsichtlich Problemstellung \ref{prob:ffCCN} eine (objekt-) relationale Umsetzung der
Vorwärtsrechnung für ein trainiertes Modell zur Ziffernerkennung vorgestellt. Schließlich werden in Kapitel \ref{kap:sum} die Ergebnisse dieser Arbeit zusammengefasst und ein Ausblick auf fortführende Forschungsthemen gegeben.