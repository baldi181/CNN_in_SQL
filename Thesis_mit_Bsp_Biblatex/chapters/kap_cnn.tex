\chapter{Gefaltete neuronale Netze}
\label{kap:CNN}

Feed-Forward-Netze gelten als leistungsstarke maschinelle Lernmethoden, da sie so trainiert werden können, um beliebige komplexe Funktionen abhängig von einer vektorwertigen Eingabe zu approximieren. Ist die Dimension der Eingabeschicht jedoch zu groß, treten bei klassischen FFN Probleme hinsichtlich der Parameteranzahl auf. Die Problemstellung \ref{prob:ffCCN} dieser Arbeit besteht in der Klassifikation digitalisierter Bilder. Wird ein FFN mit $10$ Ausgabeneuronen genutzt und jedes Pixel eines Bildes mit den Abmessungen $1000 \times 1000$ als Merkmal genutzt, so ergeben sich bereits $10^7+10$ freie Parameter. Stehen nur relativ wenige Trainingsdaten zur Verfügung, ist die Struktur des FFN zu komplex und dies kann zur Überanpassung führen \cite{caruana2000overfitting,bilbao2017overfitting}. Außerdem steigt der Zeitaufwand der Trainingsphase mit wachsender Parameteranzahl. Diese Anzahl muss also deutlich reduziert werden. 
Konzepte wie \textit{Parameter Sharing} und spärliche Konnektivität, engl. \textit{sparse connectivity}, erlauben die Reduktion der Parameteranzahl und werden ausführlich in Goodfellow \cite{Goodfellow-et-al-2016} beschrieben.

Ein weiterer Nachteil des FFN ergibt sich dadurch, dass Korrelationen von benachbarten Eingabeneuronen, z.B. Bildsegmente wie Kanten oder Ecken, nicht mit einbezogen werden. Es muss also ein Modell entwickelt werden, welches diese lokalen Muster extrahiert und sie miteinander verknüpft. Das Modell sollte zudem äquivariant gegenüber Translationen, vgl. Goodfellow \cite{Goodfellow-et-al-2016}, sein.

In diesem Kapitel wird erläutert, wie gefaltete neuronale Netze die erwähnten Nachteile von FFN umgehen. CNN sind in der Lage, lokale Muster zu erkennen, sind äquivariant gegenüber Translationen und realisieren Konzepte wie Parameter Sharing und spärliche Konnektivität, um die Anzahl der freien Parameter drastisch zu reduzieren. So gelingt es besonders bei Aufgaben der Computergrafik \cite{DBLP:conf/nips/KrizhevskySH12, DBLP:journals/pieee/LeCunBBH98,DBLP:conf/cvpr/CiresanMS12} die Generalisierungsrate gegenüber klassischen FFN zu erhöhen. An dieser Stelle sei auf Goodfellow \cite{Goodfellow-et-al-2016} verwiesen, um nachzuvollziehen, wie diese Konzepte im Detail von CNN-Modellen umgesetzt werden.

Gefaltete neuronale Netze unterscheiden sich von FFN bei der Berechnung der Übertragungsfunktion. Dazu wird die gefaltete Übertragungsfunktion definiert, welche das Konzept der diskreten Faltung nutzt. Im Abschnitt \ref{abs:conv_theorie} wird zunächst die Faltung als mathematische Operation eingeführt. 
Anschließend wird im Abschnitt \ref{abs:CNN_arch} das CNN-Modell definiert und schließlich im Abschnitt \ref{abs:CNN_train} der Backpropagationsalgorithmus \ref{alg:online_backprop} für CNN verallgemeinert. Im letzten Abschnitt \ref{abs:model_mnist} wird der Backpropagationsalgorithmus für ein konkretes Modell zur Klassifikation von Ziffern angewendet. An dieser Stelle wird der MNIST-Datensatz hinsichtlich Problemstellung \ref{prob:ffCCN} vorgestellt.

\section{Die Faltungsoperation}
\label{abs:conv_theorie}
In der Analysis ist die Faltung ein mathematischer Operator und liefert für zwei Funktionen $f$ und $g$ die Funktion $ f \ast g$, wobei mit dem Sternchen die Faltungsoperation gemeint ist.

\begin{defi}[Faltung]\label{allg_faltung}
    Für zwei Funktionen $f,g: \Rnv \rightarrow \mathbb{C}$ ist die Faltung als
    \begin{equation*}
        (f \ast g) (x) := \int_{\Rnv} f(\tau) g(x-\tau) \mathrm{d} \tau
    \end{equation*}
    definiert, wobei gefordert wird, dass das Integral für fast alle $x$ wohldefiniert ist. Für $f,g \in L^1(\RR^n)$ ist dies der Fall.
   \end{defi}

Für die Faltung gelten einige Rechenregeln.

\begin{lem}
    \label{lem:convrules}
    Seien $f,g,h \in L^1(\RR^n)$ und $a \in \mathbb{C}$. Dann gelten
    \begin{align*}
         (i) \; \; &f \ast g = g \ast f \; \; &( \text{Kommutativität}) \\
         (ii) \; \; &f \ast (g \ast h) = (f \ast g) \ast h  \; \;& (\text{Assoziativität}) \\
         (iii) \; \; &f \ast (g+h) = f \ast g + f\ast h \; \; &(\text{Distributivität}) \\ 
         (iv) \; \; &a(f \ast g) = (af) \ast g = f \ast (ag) \; \; &(\text{Assoziativität mit skalarer Multiplikation}) 
    \end{align*}
\end{lem}

\begin{proof}
    Ein Beweis dieser Rechenregeln kann in Werner\cite{werner2011funktionalanalysis} gelesen werden.
\end{proof}

%Bei klassischen neuronalen Netzen, siehe Kapitel \ref{classicNN} werden Eingabedaten durch eine Verkettung von affinen Transformationen verarbeitet. Typischerweise wird die Eingabe als Vektor dargestellt und  mit einer Matrix multipliziert, gegebenenfalls mit einem Biasvektor manipuliert und schließlich so die Ausgabe generiert. Bilder-, Audio- oder Videoaufnahmen besitzen jedoch mehrere Merkmale in unterschiedlichen Achsen. Oft sind solche Eingabedaten im Bereich des Machine-Learnings als mehrdimensionale Arrays abgelegt, welche eine oder mehrere Achsen repräsentieren, wobei die Ordnung dieser eine Rolle spielt. Bei digitalisierten Bildern sind das bespielsweise die Höhe und Breite des Bildes, bei Audioaufnahmen gibt es nur eine Achse, und zwar die Zeitachse. Hinzu kommen Kanalachsen als weitere Verfeinerung der Daten, zum Beispiel besitzen RGB-Farbbilder drei Kanäle der Farben rot, grün und blau. 

%Diese speziellen Eigenschaften können bei affinen Transformationen nicht berücksichtigt werden. Alle Merkmale sowie Achsen werden gewissermaßen gleich behandelt und die wesentliche topologische Struktur kann so nicht zum Vorteil ausgenutzt werden. Hier soll nun die sogenannte diskrete Faltung Abhilfe schaffen.
In der digitalen Signal- und Bildverarbeitung werden meist diskrete Funktionen analysiert und daher die diskrete Faltung genutzt, bei der statt der Integration eine Summation auftaucht. Die Regeln aus Lemma \ref{lem:convrules} gelten analog.

\begin{defi}[Diskrete Faltung]\label{disk_faltung}
    Für zwei Funktionen $f,g: D \rightarrow \mathbb{C}$ mit einem diskreten Definitionsbereich $D \subseteq \mathbb{Z}^n$ ist die diskrete Faltung als
    \begin{equation*}
        (f \ast g) (n) := \sum_{k \in D} f(k) g(x-k)
    \end{equation*}
    definiert. Hier wird über den gesamten Definitionsbereich $D$ summiert. Ist $D$ beschränkt, werden $f$ beziehungsweise $g$ durch Nullen fortgesetzt.  
\end{defi}

Im Hinblick auf die Klassifikation von digitalisierten Bildern, dargestellt als Matrizen, wird die Matrixfaltung mit sogenannten quadratischen Kernen $K \in \RR^{k \times k}$ mit ungeradem $k \in \mathbb{N}$ definiert.
 
%Um die Notation einfach zu halten, sei ein Kern $K$ ebenfalls eine $h \times b$ - Matrix, indem $K$ mit Nullen aufgefüllt wird.

%\begin{defi}[Zweidimensionale Faltung]
    %Sei $X \in \RR^{h \times b}$ und $K \in \RR^{k \times k}$. Das Ergebnis der zweidimensionalen Faltung ist die Matrix $Y= X \ast K \in \RR^{h \times b}$ mit

%    \begin{equation}
%        \label{eq:2dmatrixconv}
%        Y_{i,j}= \sum_{p \in [k]} \sum_{q \in [k]} X_{p,q} K_{p-i+3,q-j+3}.
%    \end{equation}
%    Dabei besitzt $Y$ die selben Abmessungen wie $X$. 
%\end{defi}

\begin{defi}[Matrixfaltung, vgl. \cite{gruening}] \label{def:matrix_faltung}
    Seien die Matrizen $X \in \RR^{h \times b}$ und $K \in \RR^{k \times k}$ gegeben. Sei $l=\lfloor k/2  \rfloor$.
    %\begin{equation*}
    %   h_l=\begin{cases}
    %       \lfloor k_h/2  \rfloor &, k_h \, \text{ungerade} \\
    %       k_h/2-1 &, \text{sonst}
    %    \end{cases}, \; \; 
    %    w_l=\begin{cases}
    %        \lfloor k_w/2 \rfloor &, k_w \, \text{ungerade} \\
    %        k_w/2 -1 &, \text{sonst}    
    %    \end{cases}.
    %\end{equation*} 
    Die Matrixfaltung  $Y=X \ast K \in \RR^{h \times w}$ ist als 
    \begin{equation}
        \label{eq:matrix_faltung}
        Y_{i,j}:=\sum_{u=-l}^{l} \sum_{v=-l}^{l} X_{i+u,j+v} K_{u+l+1,v+l+1}\; \; \forall i \in [h], j \in [b]
    \end{equation} mit $X_{i,j}=0$ für $i \notin [h]$ und $j \notin [b]$ definiert. In dieser Definition besitzt das Ergebnis $Y$ der Faltung die gleichen Abmessungen wie $X$.
\end{defi}

\begin{bem}
    \label{bem:K_conv_komp}
    Oft wird eine kompakte Schreibweise der Faltung von $X \in \RR^{h \times b}$ und $ K \in \RR^{k \times k}$ mit 
    \begin{equation}
        \label{eq:valid_conv}
        Y_{i,j}=\sum_{u=-l}^l \sum_{v=-l}^l X_{i-u,j-v}K_{u,v},
    \end{equation} beziehungsweise der Kreuzkorrelation mit
    \begin{equation}
        \label{eq:valid_cross}
        Y_{i,j}=\sum_{u=-l}^l \sum_{v=-l}^l X_{i+u,j+v}K_{u,v}
    \end{equation}
    und $X_{i,j}=0$ für $i \notin [h]$ und $j \notin [b]$ angegeben. Das Auffüllen mit Nullen am Rand von $X$ wird mit \textit{zero padding} bezeichnet. Die Kreuzkorrelation (\ref{eq:valid_cross}), in der Fachliteratur oft auch als Faltung bezeichnet, stellt die Faltung (\ref{eq:valid_conv}) mit gedrehtem Kern $K^{rot180}$ dar. Dabei ist 
    \begin{equation*}
        K^{rot180}_{-u,-v}:=K_{u,v}.
    \end{equation*}
    Hier werden die Kerne speziell indiziert. Ist beispielsweise $k=3$, so ist $K \in \RR^{3 \times 3}$ durch
    \begin{equation*}
        K=\begin{pmatrix}
            K_{-1,-1} &K_{-1,0} &K_{-1,1} \\
            K_{0,-1} &K_{0,0} &K_{0,1}  \\
            K_{1,-1} &K_{1,0} &K_{1,1} 
        \end{pmatrix}
    \end{equation*} und $K^{rot180}$ durch

    \begin{equation*}
        K^{rot180}=\begin{pmatrix}
            K_{1,1} &K_{1,0} &K_{1,1} \\
            K_{0,1} &K_{0,0} &K_{0,-1}  \\
            K_{-1,1} &K_{-1,0} &K_{-1,-1} 
        \end{pmatrix}
    \end{equation*}
    gegeben. In dieser Arbeit wird (\ref{eq:valid_cross}) als Matrixfaltung bezeichnet.
\end{bem}

Bei gefalteten neuronalen Netzen wird oft eine Reduktion der Dimensionen angestrebt. Dafür werden natürliche Zahlen als Schrittweiten, engl. \textit{strides}, genutzt.
\begin{bem}\label{bem_strides}
    Für Schrittweiten $s_h, s_b \in \mathbb{N}$ ist die reduzierte Matrixfaltung $Y=X \ast K$ als
    \begin{equation*}
        Y_{i,j}:=\sum_{u=-l}^{l} \sum_{v=-l}^{l} X_{i \cdot s_h +u,j \cdot s_b +v} K_{u+l+1, v+l+1}\; \; \forall i \in [\lceil h/s_h \rceil], j \in [\lceil b/s_b \rceil]
    \end{equation*}
    definiert.
    Für $s_h=s_b=1$ ergibt sich die Standardvariante wie in \ref{eq:matrix_faltung}.
    \end{bem}

\section{CNN-Architektur}
\label{abs:CNN_arch}
Beim maschinellen Lernen sind Eingabedaten oft als mehrdimensionale Arrays dargestellt, welche eine oder mehrere Achsen repräsentieren, wobei die Ordnung dieser eine Rolle spielt. Bei digitalisierten Bildern sind das beispielsweise die Höhe und Breite des Bildes, welche als Raumachsen bezeichnet werden. Hinzu kommen Kanalachsen, zum Beispiel besitzen Grauwert-Bilder einen Farbkanal, während RGB-Farbbilder drei Kanäle der Farben Rot usw. besitzen. Dementsprechend werden Grauwert-Bilder wie in Definition \ref{def:image} nun als dreidimensionale Arrays $X \in [0,1]^{h \times b \times 1}$ dargestellt. Dies erlaubt die Definition der gefalteten Übertragungsfunktion, wie in Gruening \cite{gruening}.

\begin{defi}[Gefaltete Übertragungsfunktion]
    \label{eq:convlogit}
    Sei ein vierdimensionales Array, in Zukunft oft Kern genannt, $K \in \RR^{z_{out} \times z_{in} \times k \times k}$ und ein Biasvektor $b \in \RR^{z_{out}}$ gegeben. Die Funktion 
    \begin{equation*}
        \Psi_{conv}^{K,b}: \RR^{\cdot \times \cdot \times z_{in}} \rightarrow \RR^{\cdot \times \cdot\times z_{out}}
    \end{equation*}
    mit
    \begin{equation*}
        \Psi_{conv}^{K,b}(X)_{:,:,q}:= \sum_{p=1}^{z_{in}} \alpha_{qp} \left(K_{q,p,:,:} \ast X_{:,:,p} \right) +b_q \; \forall q \in [z_{out}]
    \end{equation*}
    wird gefaltete Übertragungsfunktion bezeichnet. Mit $\ast$ ist die Matrixfaltung wie in Definition \ref{def:matrix_faltung} gemeint und mit $\cdot$ werden beliebige Raumachsen bezeichnet. Das Ergebnis der Übertragungsfunktion wird Netzeingabe genannt. Die Skalare $\alpha_{qp}$ können als lernbare Parameter genutzt werden. In dieser Arbeit gelte $\alpha_{qp}=1$ für alle $q \in [z_{out}]$ und $p \in [z_{in}]$.
\end{defi}


\begin{bem}
    Ist $\psi: \RR \rightarrow \RR$ eine Aktivierungsfunktion wie in Definition \ref{def_act_f}, so wird für $X \in \RR^{\cdot \times \cdot \times z}$ mit 
    \[\psi(X)_{i,j,:}:=\left(\psi(X_{i,j,1}), \ldots, \psi(X_{i,j,z})\right)^T \in \RR^z \; \; \forall i \in [{}_1 X], j \in [{}_2 X] 
    \]
    der Vektor bezeichnet, welcher sich durch die elementweise Auswertung der Aktivierungsfunktion $\psi$ ergibt.
\end{bem}

Ähnlich der Definition \ref{def:NNlayer} wird nun eine Faltungsschicht als Verknüpfung von gefalteter Übertragungsfunktion und Aktivierungsfunktion definiert.

\begin{defi}[Faltungsschicht]
    \label{def:convlayer}
    Ist $\Psi_{conv}^{K,b}$ eine gefaltete Übertragungsfunktion und $\psi$ eine Aktivierungsfunktion, so wird das Paar $(\Psi_{conv}^{K,b}, \psi)$ als Faltungsschicht $\mathcal{S}_{conv}$ bezeichnet. Für eine sogenannte Eingabekarte $X \in \RR^{\cdot \times \cdot \times z_{in}}$ ist die Ausgabe $Y \in \RR^{\cdot \times \cdot \times z_{out}}$ der Schicht $\mathcal{S}_{conv}$ durch
    \[Y=\psi \circ \Psi_{conv}^{K,b}(X)= \psi\left(\Psi_{conv}^{K,b}(X)\right)
        \] 
        gegeben. Die Matrizen $Y_{:,:,p}$ werden für $1 \leq p \leq z_{out}$ Merkmalskarten genannt. Weiter bezeichne $\Psi_{conv}^{K^{},b^{},\psi_{}}$ die Faltungsschicht $\mathcal{S}_{conv}$ mit $\Psi_{conv}^{K^{},b^{},\psi_{}}(X):= \psi_{} \left(\Psi_{conv}^{K^{},b^{}}(X)\right)$.
\end{defi}

Bei CNN werden sogenannte \textit{Pooling}-Schichten verwendet, um die Dimensionen der Raumachsen neben dem zero padding weiter zu verkleinern und das Modell robuster gegenüber Überanpassung zu machen. Dazu werden Pooling-Funktionen eingesetzt, welche unabhängig voneinander auf Merkmalskarten operieren und so die Rechenkomplexität des Modells reduzieren. Es ist sinnvoll, symmetrische Pooling-Funktionen $T$ zu wählen, für die die Bedingung
\begin{equation*}
    \forall \pi \in S_n \; \forall x \in \RR^n: T(x_1, \ldots, x_n)=T(x_{\pi(1)}, \ldots, x_{\pi(n)})
\end{equation*}
gilt. Mögliche Pooling-Funktionen sind 
\begin{alignat*}{2}
    \text{Maximum}: \; \;&T(x_1, \ldots x_n) &&= \max\{x_1, \ldots x_n\},\\
    \text{Mittelwert}: \; \;&T(x_1, \ldots, x_n) &&= \frac{1}{n} \sum_{i=1}^n x_i. 
 \end{alignat*}

Für den späteren Trainingsprozess ist es nützlich, die Ableitung der verwendeten Pooling-Funktionen, sofern sie existiert, zur Verfügung zu haben. Für den Mittelwert $T$ gilt $\nabla T=\frac{1}{n} \textbf{1}$. Ist $T$ die Maximum-Funktion so ergibt sich

\begin{equation*}
    \frac{\partial T}{\partial x_i}= \begin{cases}
        1 , \; \text{falls}  \; \forall j \neq i: \; x_i >x_j \\
        0 , \; \text{falls}  \; \exists j \neq i: \; x_i <x_j
    \end{cases}
\end{equation*}
mit der Konvention, dass für $x_1=x_2= \ldots=x_n$ die Ableitung auf $\frac{1}{2}
$ gesetzt wird. Pooling-Schichten werden wieder durch Schrittweiten parametrisiert.

\begin{defi}[Pooling-Schicht, vgl. Grüning \cite{gruening}]
    Seien $p_h, p_b \in \mathbb{N}$ und $T$ eine Pooling-Funktion. Die Funktion 
    \begin{equation*}
        \Psi_{pool,T}^{p_h,p_b}: \RR^{\cdot \times \cdot \times z} \rightarrow \RR^{\cdot \times \cdot\times z}
    \end{equation*}
    mit
    \begin{equation*}
        \Psi_{pool,T}^{p_h,p_b}(X)_{i,j,l}:= \mathop{T}_{\begin{subarray}{l}(i-1) \cdot p_h < i' \leq \min\{i \cdot p_h, {}_1 X\} \\ 
        (j-1) \cdot p_w < j' \leq \min\{j \cdot p_w, {}_2 X\}\end{subarray}} (X_{i',j',l})  
    \end{equation*}
    für alle  $i \in [\lceil {}_1 X/p_h \rceil], j \in [\lceil {}_2 X/p_w \rceil]$ und $l \in [z]$ wird Pooling-Schicht genannt. Die Schrittweiten $p_h, p_b \in \mathbb{N}$ werden subsampling-Faktoren genannt.
\end{defi}

Pooling-Schichten verdichten also Informationen von Eingabedaten, welche sich lokal in Fenstern der Größe $p_h \times p_b$ befinden und reduzieren so die Raumdimension. In dieser Arbeit wird das Maximum-Pooling oder Mittelwert-Pooling benutzt. Für andere Pooling-Funktionen sei auf Yu et. al.\cite{yu2014mixed} verwiesen.
Die Idee bei CNN besteht nun darin, Faltungsschichten mit Pooling-Schichten zu kombinieren und schließlich ein FFN wie in Definition \ref{def:MLP} anzuknüpfen.
Dazu wird für allgemeine mehrdimensionale Arrays die Flatten-Funktion definiert.

\begin{defi}
    \label{def:flatten}
    Sei $X \in \RR^{n_1 \times n_2 \times n_3}$. Dann wird die Funktion $T_f:\RR^{n_1 \times n_2 \times n_3} \rightarrow \RR^{n_1 \cdot n_2 \cdot n_3}$ mit 
    \begin{equation*}
        T_f(X)_{(i-1) \cdot (n_2 \cdot n_3)+(j-1) \cdot n_3+k}:= X_{i,j,k}, \; \; \forall i \in [n_1],\; j \in [n_2],\; k \in [n_3]
    \end{equation*}
    Flatten-Funktion genannt. Die mehrdimensionale Eingabe wird also in einen Vektor umgewandelt.
\end{defi}

Dies erlaubt die Definition des CNN-Modells als Kombination aus Faltungsschichten, Pooling-Schichten und Neuronenschichten eines FFN.
\begin{defi}(Gefaltetes Neuronales Netz)
    \label{def_fw_cnn}
    Seien $h, w, z_{in}, z_{out}, l ,c \in \mathbb{N}$ und Schichten $\Psi^{W^{(1)},b^{(1)},\psi_{1}}, \ldots, \Psi^{W^{(l)},b^{(l)},\psi_{l}}$ sowie Faltungsschichten $\Psi_{conv}^{K^{(1)},b^{(1)},\psi_{1}}, \ldots, \Psi_{conv}^{K^{(l)},b^{(l)},\psi_{l}}$ gegeben. Die Vorwärtsrechnung eines CNN lässt sich als Komposition
    \begin{equation}
        \label{eq:CNN_for}
        y=\Psi^{W^{(l)},b^{(l)},\psi_{l}} \circ \ldots \circ \Psi^{W^{(1)},b^{(1)},\psi_{1}} \circ T_f \circ \Psi_{conv}^{K^{(c)},b^{(c)},\psi_{c}} \circ \ldots \circ \Psi_{conv}^{K^{(1)},b^{(1)},\psi_{l}}(X)
    \end{equation} 
    darstellen. Dabei seien wieder die Dimensionen der Parameter passend gewählt, sodass die Komposition wohldefiniert ist. In (\ref{eq:CNN_for}) können zwischen den Faltungsschichten Pooling-Schichten $\Psi_{pool,T}^{p_h,p_b}(X)$ geschaltet werden.
\end{defi}
Auch CNN bestehen aus Hyper- und Modellparametern.

\begin{defi}[Hyper- und Modellparameter von CNN]
    Die Eingabe- und Ausgabedimensionen $h, b, z_{in}, z_{out}$, die Anzahl $c$ der Faltungsschichten, die Anzahl $l$ der Neuronenschichten, die Dimensionen der Kerne, die Schrittweiten bei der Faltung bzw. dem Pooling sowie die verwendeten Aktivierungs- und Poolingfunktionen sind Hyperparameter des CNN.
    Die Kerne, Gewichtsmatrizen und Biasvektoren mit den entsprechend passenden Abmessungen stellen die Modellparameter $\mathcal{W}:=\{(W^{(i)},b^{(i)}): \; i=1, \ldots, l\}$ und $\mathcal{W}_{conv}:=\{(K^{(i)}, b^{(i)}): \; i=1, \ldots, c\}$ des CNN dar. 
\end{defi}
Die Hyperparameter werden wieder anwendungsspezifisch für das jeweilige Problem gewählt und die Modellparameter während des Trainingsprozesses angepasst. 

\section{Backpropagation bei CNN}
\label{abs:CNN_train}
Der Backpropagationsalgorithmus \ref{alg:online_backprop} soll nun für das CNN-Modell verallgemeinert werden. 
Als Fehlerfunktion wird wieder die mittlere quadratische Abweichung (\ref{eq:MSE_single})
genutzt. 
Die Abstiegsrichtungen für die Schichten des FFN wurden bereits im vorherigen Kapitel \ref{kap:NN} hergeleitet. Es müssen nun Gradienten innerhalb der Faltungsschichten beziehungsweise Pooling-Schichten berechnet werden. Die Einträge der Eingabe- und Merkmalskarten, in Zukunft einfach Karten genannt, können als Neuronenaktivierungen interpretiert werden. 

Zur Vereinfachung werden die trainierbaren Parameter der Kerne und Gewichtsmatrizen als Gewichte $w_{j,i}^{(l)}$ bezeichnet. Gemeint ist also das Gewicht zwischen Neuron $i$  und Neuron $j$ in der (Faltungs)-Schicht $l$. Weiter sei $y_{i}^{(l-1)}$ die Aktivierung des Neurons $i$ der (Faltungs)-Schicht $l-1$ und $\delta_j^{(l)}$ sei der lokale Fehler des Neurons $j$ der (Faltungs)-Schicht $l$, welcher im Folgenden verallgemeinert wird. Die Funktion $\psi$ repräsentiere eine Aktivierungsfunktion einer  (Faltungs)-Schicht und Pooling-Funktionen werden mit $T$ sowie Flatten-Funktion mit $T_f$ bezeichnet.
%\begin{alignat*}{3}
   % &w_{j,i}^{(l)}  &&\text{Gewicht zwischen Neuron $i$ und Neuron $j$}, \\
    %&y_{j}^{(l-1)}   &&\text{Aktivierung des Neurons $i$ der Schicht $l-1$}, \\
    %&V_k^{(l)}=\sum_{j} w_{k,j}^{(l)} y^{(l-1)}_j +b^{(l)}_k \; \; &&\text{Netzeingabe des Neurons $k$ der Schicht $l$}, \\
    %&y_k &&\text{Aktivierung des Ausgabeneurons $k$},\\
   % &e_k=y_k-t_k &&\text{Fehler des $k$-ten Ausgabeneurons}, \\
    %&\delta_k^{(l)}=e_k \psi' (V_k^{(l)})\; \; \;  &&\text{lokaler Fehler des Neurons $k$ auf Schicht $l$}. 
%\end{alignat*}

Die Gradienten werden wieder komponentenweise berechnet.
Angenommen, $l$ ist eine Faltungsschicht. Die Aktivierung einer Merkmalskarte $j$ an der Stelle $(x,y)$ lässt sich mit der Matrixfaltung, siehe Bemerkung \ref{bem:K_conv_komp}, komponentenweise als
\begin{equation}
    y_j^{(l)}(x,y) =\psi \left(\sum_{i=1}^{z_{in}} \sum_{(u,v) \in F} y_i^{(l)}(x+u,y+u)\,  w_{j,i}^{(l)}(u,v) +b_j^{(l)}\right)
\end{equation}   
schreiben. Es ist zu beachten, dass $(x,y)$ hier ein Pixel der Karte $j$ meint und nicht mit Eingabe- bzw. Ausgabevektoren zu verwechseln ist. Mit $k$ als Dimension des Kerns und $l=\lfloor k/2 \rfloor$ ist $F=\{(u,v): \; -l \leq u,v \leq l\}$.
Das Gewicht des Kerns von der Karte $i$ zur Karte $j$ an der Stelle $(u,v)$ ist mit $w^{(l)}_{j,i}(u,v)$ bezeichnet. Der Gradient von $w_{j,i}^{(l)}$ ergibt sich als Summe über alle beteiligten Pixel $(x,y)$ der Merkmalskarte $j$, also
\begin{equation}
    \label{eq:CNN_K_grad}
    \Delta w_{j,i}^{(l)}(u,v)= \sum_{(x,y)} \left(\delta_j^{(l)}(x,y)  y_i^{(l-1)}(x+u,y+v)\right).
\end{equation}
Der Zusammenhang in Gleichung (\ref{eq:CNN_K_grad}) ist in Abbildung \ref{abb:error_conv} grafisch dargestellt. Analog ergibt sich für den Schwellwert

\begin{equation*}
    \Delta b_j^{(l)}= \sum_{(x,y)} \delta_j^{(l)}(x,y).
\end{equation*}

\begin{figure}[h]
    \includegraphics[width=0.8\textwidth]{pics/chapters/CCN/backprop_dudiss.png}
    \centering
    \caption[Rückwärtsrechnung bei CNN]{Es ist die Rückwärtsrechnung bei Faltungsschichten dargestellt. Die Abbildung wurde aus \cite{du_diss} entnommen.}
    \label{abb:error_conv}
\end{figure}
Die Berechnung des lokalen Fehlers $\delta_{j}^{(l)}$ in Gleichung (\ref{eq:CNN_K_grad}) der Schicht $l$ ist abhängig von der Schicht $l+1$. Auch hier wird wieder komponentenweise vorgegangen. Im Folgenden bezeichne $V_j^{(l)}$ die Netzeingabe des Neurons $j$ auf der Schicht $l$.
Ist die Schicht $l+1$ eine Neuronenschicht mit $s_k$ Neuronen, wird der lokale Fehler \ref{eq:delta_hidden} zu
\begin{equation*}
    \delta_j^{(l)}(x,y)=\sum_{k=1}^{s_{k}} \left(\sum_{(x,y)} \delta_k^{(l+1)} w_{k,j}^{(l+1)}(x,y)\right) \psi'(V_j^{(l)})
\end{equation*}
verallgemeinert. 
Hier ist $\delta_j^{(l+1)}$ der von der Neuronenschicht $l+1$ zurück propagierte Fehler des Neurons $j$ bezüglich der Fehlerfunktion $E$ und $\psi(V^{(l)}_j)$ die entsprechende Aktivierung des Neurons $j$.
Ist die nachfolgende Schicht eine Faltungsschicht, so ergibt sich der lokale Fehler der Schicht $l$ zu
\begin{equation*}
    \delta_j^{(l)}(x,y)=\sum_{k=1}^{z_{out}} \left(\sum_{(u,v) \in F} \delta_k^{(l+1)}(x,y) w_{k,j}^{(l+1)}(u,v)\right) \psi'(V_j^{(l)}).
\end{equation*}
Ist $l+1$ eine Pooling-Schicht mit Schrittweiten $p_h,p_b$ und einer Pooling-Funktion $T$, so gilt
\begin{equation}
    \label{eq:upsample1}
    \delta_j^{(l)}(x,y)=\delta_j^{(l+1)}(\lceil x/p_h \rceil ,\lceil y/p_b \rceil)\nabla T.
\end{equation}
Dabei wird der Gradient $\nabla T$ jeweils an der zu $(x,y)$ korrespondierenden Stelle ausgewertet und mit dem Fehler der darüberliegenden Schicht multipliziert.
Der lokale Fehler wird je nach Wahl der Pooling-Funktion zurück propagiert. Beim Mittelwert-Pooling wird der Fehler gleichmäßig auf alle beteiligten Pixel aufgeteilt. Beim Maximum-Pooling wird der Fehler lediglich auf die Position des Neurons zurück propagiert, welches die größte Aktivierung besitzt.
Ist $l+1$ eine Flatten-Schicht mit einer Flatten-Funktion $T_f$, so gilt
\begin{equation}
    \label{eq:upsample2}
    \delta_j^{(l)}=T^{-1}_f(\delta_j^{(l+1)}).
\end{equation}
Die Berechnung in Gleichung (\ref{eq:upsample1}) und Gleichung (\ref{eq:upsample2}) wird Upsampling genannt. Die obigen Resultate sind im Algorithmus \ref{alg:cnn_online} zusammengefasst. Hier beschreibe $L$ die Anzahl aller Faltungsschichten, Pooling-Schichten und Neuronenschichten zusammen.

\begin{algorithm}[H]
    \scriptsize
    \caption{Online-Backpropagation für gefaltete neuronale Netze, vgl. \cite{du_diss}}
    \label{alg:cnn_online}
    \begin{algorithmic}
    \Require  Trainingsmenge $\mathcal{T}$, Modellparameter $\mathcal{W}$ und $\mathcal{W}_{conv}$, Fehlerfunktion $E$, Lernrate $\lambda$ 
    \Ensure $\text{optimierte Modellparameter} \; \mathcal{W}, \mathcal{W}_{conv}$
    \State Initialisiere zufällig alle Gewichte und Schwellwerte des CNN 
    \State Berechne für die Eingabe $(x,c)$ den Zielvektor $t=t(x,c)$
    \State $n=0$  
    \State $E=\inf$
    \While{$E >\varepsilon$ \textbf{or} $n < N$} \Comment{Abbruchbedingung wie bei Algorithmus \ref{alg:online_backprop}}
    \For{$(x,c) \in \mathcal{T}$}
        \State Vorwärtsrechnung $y$
        \For{$k=1, \ldots, s_L$}
            \State $\delta_k^{(L)}=(y_k-t_k) \psi'(V^{(L)}_k)$
        \EndFor
        \For{Schichten $l=L-1, \ldots 1$}
             \For{\textbf{all} Karten/Neuronen $j$ in Schicht $l$}
                \For{\textbf{all} Pixel $(x,y)$}
                    \If{Schicht $l+1$ ist Neuronenschicht}
                    \State $\delta_j^{(l)}(x,y)=\sum_{k=1}^{s_{l+1}} \left(\sum_{(x,y)} \delta_k^{(l+1)} w_{k,j}^{(l+1)}(x,y)\right) \psi'(V_j^{(l)})$
                    \EndIf

                    \If{Schicht $l+1$ ist Faltungsschicht}
                    \State $\delta_j^{(l)}(x,y)=\sum_{k=1}^{z_{out}} \left(\sum_{(u,v) \in F} \delta_k^{(l+1)}(x,y) w_{k,j}^{(l+1)}(u,v)\right) \psi'(V_j^{(l)})$
                    \EndIf

                    \If{Schicht $l+1$ ist Pooling-Schicht}
                    \State $\delta_j^{(l)}(x,y)=\delta_j^{(l+1)}(\lceil x/p_h \rceil ,\lceil y/p_b \rceil) \nabla T$
                    \EndIf

                    \If{Schicht $l+1$ ist Flatten-Schicht}
                    \State $ \delta_j^{(l)}=T^{-1}_f(\delta_j^{(l+1)})$
                    \EndIf
                \EndFor
             \EndFor
        \EndFor 
        \State Berechne Gradienten und aktualisiere Gewichte
        \For{$l=1, \ldots, L$}
            \If{Schicht $l$ ist Neuronenschicht}

                \For{$j=1, \ldots, s_{l}$}
                    \For{$i=1, \ldots s_{j-1}$}
                        \State $\Delta w_{j,i}^{(l)}= \delta_j^{(l)} y^{(l-1)}_i$
                        \State $w^{(l)}_{j,i}= w^{(l)}_{j,i} + \lambda \Delta w_{j,i}^{(l)}$
                    \EndFor
                    \State $\Delta b^{(l)}_j=\delta^{(l)}_j$
                    \State $b^{(l)}_j=b^{(l)}_j + \lambda \Delta b^{(l)}_j$
                \EndFor
            \EndIf

            \If{Schicht $l$ ist Faltungsschicht}

                \For{$j=1, \ldots, z_{out}$}
                    \For{$i=1, \ldots z_{in}$}
                        \For{$(u,v) \in F$}
                        \State $\Delta w_{j,i}^{(l)}(u,v)= \sum_{(x,y)} \left(\delta_j^{(l)}(x,y)  y_i^{(l-1)}(x+u,y+v)\right)$
                        \State $w^{(l)}_{j,i}= w^{(l)}_{j,i} + \lambda \Delta w_{j,i}^{(l)}$
                        \EndFor
                    \EndFor
                    \State $\Delta b^{(l)}_j=\sum_{(x,y)} \delta^{(l)}_j(x,y)$
                    \State $b^{(l)}_j=b^{(l)}_j + \lambda \Delta b^{(l)}_j$
                \EndFor
            \EndIf
        \EndFor    
        \State $n=n+1$
    \EndFor
    \State $E=\frac{1}{2} \sum_{(x,c) \in \mathcal{T}} ||y-t||_2^2$
    \EndWhile
    \end{algorithmic}
\end{algorithm}


\section{Anwendung bei der Ziffernerkennung}
\label{abs:model_mnist}
In diesem Abschnitt wird ein CNN zur Klassifikation von Grauwert-Bildern aus einem konkreten Datensatz vorgestellt. Der MNIST-Datensatz\cite{DBLP:journals/pieee/LeCunBBH98} bietet $60.000$ Trainingsbilder handgeschriebener Ziffern und $10.000$ Testbilder, welche jeweils durch menschliches Wissen annotiert sind. Dieser Datensatz gilt als typischer Benchmark zur Klassifikation von Ziffern und wird im weiteren Verlauf dieser Arbeit genutzt. Jedes einzelne Bild besteht aus $28 \times 28$ Pixeln, welche jeweils einen Grauwert zwischen 0 und 1 annehmen, vgl. Abbildung \ref{mnistpic}. Ein Bild wird als $X \in [0,1]^{28 \times 28 \times 1}$ und ein Trainingspaar als $(X,c)$ mit $c \in \{0, \ldots, 9\}$ bezeichnet. 

\begin{figure}[h]
    \includegraphics[width=0.8\textwidth]{pics/chapters/CCN/mnist.png}
    \centering
    \caption[MNIST-Datensatz]{Beispielbilder, vgl. \cite{DBLP:journals/pieee/LeCunBBH98} aus der öffentlichen MNIST-Datenbank. Zu sehen sind handgeschriebene Ziffern und zugehörige Annotationen.}
    \label{mnistpic}
\end{figure}

Es wird ein CNN mit zwei Faltungsschichten $C^1$ und $C^2$ mit der logistischen Aktivierungsfunktion $\psi$, auch \textit{sigmoid} genannt, genutzt. Die Ausgabe der Schicht $C^1$ umfasst sechs Merkmalskarten und die Ausgabe der Schicht $C^2$ zwölf Merkmalskarten. Zur Faltung werden quadratische $5 \times 5$ -Kerne verwendet. Außerdem werden zwei Pooling-Schichten $P^1$ und $P^2$ mit den Schrittweiten $p_h=p_b=2$ und der Mittelwert-Funktion genutzt. Anschließend wird ein FFN mit 10 Ausgabeneuronen angekoppelt, sodass dessen Aktivierung zur Klassifikation der Eingabe genutzt werden kann. Die Architektur, ähnlich dem Le-Net-5-Modell\cite{DBLP:journals/pieee/LeCunBBH98}, ist in Abbildung \ref{modell} dargestellt. Dabei sind

\begin{itemize}
    \item $X$ ein Grauwert-Bild der Größe $h=b=28$,
    \item $K^1 \in \RR^{6 \times 1 \times 5 \times 5}$ der Kern der Schicht $C^1$,
    \item $b^1 \in \RR^6$ der Biasvektor der Schicht $C^1$,
    \item $K^2 \in \RR^{12 \times 6 \times 5 \times 5}$ der Kern der Schicht $C^2$,
    \item $b^2 \in \RR^{12}$ der Biasvektor der Schicht $C^2$,
    \item $W \in \RR^{192 \times 10}$ die Gewichtsmatrix der Neuronenschicht,
    \item $b \in \RR^{10}$ der Biasvektor der Neuronenschicht,
    \item $y \in \RR^{10}$ die Ausgabe des CNN. 
\end{itemize}

\begin{figure}[h]
    \includegraphics[width=1.0\textwidth]{pics/chapters/CCN/modell_arch_anp.png}
    \centering
    \caption[Das in dieser Arbeit verwendete CNN-Modell]{Das CNN-Modell zur Klassifikation von handgeschriebenen Ziffern ist dargestellt. Die Abbildung wurde \cite{gentle} entnommenen und angepasst.}
    \label{modell}
\end{figure}

Im Folgenden wird der Online-Backpropagationsalgorithmus \ref{alg:cnn_online} verwendet, um das CNN zu trainieren. Dieser besteht zunächst aus der Initialisierung und der Vorwärtsrechnung. Zur Übersicht wird im Folgenden eine komponentenweise Notation genutzt, die sich an Zhang \cite{key} orientiert. 

\subsection*{Vorwärtsrechnung}
Alle Biasvektoren werden als Nullvektor initialisiert. Die Initialisierung der Gewichte wird ähnlich der Xavier-Initialisierung \cite{DBLP:journals/jmlr/GlorotB10} vorgenommen, also

\begin{alignat*}{2}
    K^{1}_{p,1}(u,v) &\sim  \mathcal{N} \left(0, \frac{2}{5 \cdot 5 \cdot (1+6)}\right), \; \; && 1 \leq p \leq 6,\\
    K^{2}_{q,p}(u,v) &\sim  \mathcal{N} \left(0, \frac{2}{5 \cdot 5 \cdot (6+12)}\right),\; \; && 1 \leq q \leq 12, 1 \leq p \leq 6,\\
    W(i,j) &\sim \mathcal{N} \left(0, \frac{2}{192+10}\right), \; \; &&1 \leq i \leq 192 , 1 \leq j \leq 10.
\end{alignat*}
Die Parameteranzahl ist damit 
\begin{equation*}
    \underbrace{(5 \cdot 5 +1) \cdot 6}_{\text{Gewichte in $C^1$}}+\underbrace{(5 \cdot 5 \cdot 6+1) \cdot 12}_{\text{Gewichte in $C^2$}}+\underbrace{(192+1 \cdot 10)}_{\text{Gewichte im FFN}}=3898.
\end{equation*}
 Sei $X \in \RR^{28 \times 28 \times 1}$ das Eingabebild und $\psi$ die logistische Funktion. In der Schicht $C^1$ werden sechs Merkmalskarten 
\begin{align*}
    \label{eq:C1_forw}
    \begin{split}
    C_p^1 &= \psi(X \ast K_{p,1}^1+b_p^1 \mathbf{1}) \\
    C_p^1(i,j)&=\psi \left(\sum_{u=-2}^2 \sum_{v=-2}^2 X(i+u,j+v) \cdot K_{p,1}^1(u,v) +b_p^1\right)
    \end{split}
\end{align*}
für $1 \leq p \leq 6$ berechnet. Mit $\mathbf{1} \in \RR^{24 \times 24}$ ist die Matrix gemeint, deren Einträge nur Einsen sind. Die Matrixfaltung $\ast$ wird ohne zero padding genutzt und daher reduziert sich die Raumdimension. Es werden also nur die Teilergebnisse genutzt, bei denen die Filter vollständig in der Eingabekarte liegen. Diese Methode wird gültige Faltung, engl. \textit{valid convolution}, genannt. Dann gilt $C^1 \in \RR^{6 \times 24 \times 24}$. Anschließend folgen das Mittelwert-Pooling $P^1$ mit
\begin{equation*}
    \label{eq:P1_forw}
    P^1_p(i,j) =\frac{1}{4} \sum_{u=0}^1 \sum_{v=0}^1 C_p^1(2i-u, 2j-v), \; 1 \leq i,j \leq 12
\end{equation*}
für $1 \leq p \leq 6$. Es gilt $P^1 \in \RR^{6 \times 12 \times 12}$. Es folgt die zweite Faltungsschicht $C^2$, welche zwölf Merkmalskarten
\begin{align*}
    \label{eq:C2_forw}
    \begin{split}
        C_q^2 &= \psi \left( \sum_{p=1}^6 P_p^1 \ast K_{q,p}^2 + \mathbf{1} b_q^2\right) \\
        C_q^2(i,j) &= \psi \left( \sum_{p=1}^6 \sum_{u=-2}^2 \sum_{v=-2}^2 P^1_p(i+u,j+v) \cdot K^2_{q,p}(u,v) +b_q^2\right)
    \end{split}
\end{align*}
für $1 \leq q \leq 12$ berechnet. Die Matrixfaltung $\ast$ wird wieder ohne zero padding genutzt und daher reduziert sich die Raumdimension.
Es gilt $C^2 \in \RR^{12 \times 8 \times 8}$. Es folgt das Mittelwert-Pooling $P^2$ mit
\begin{equation*}
    \label{eq:P2_forw}
    P_q^2(i,j)= \frac{1}{4} \sum_{u=0}^1 \sum_{v=0}^1 C_p^1(2i-u, 2j-v), \; 1 \leq i,j \leq 4
\end{equation*}
für $1 \leq q \leq 12$. Dann gilt $P^2\in \RR^{12 \times 4 \times 4}$. Diese zwölf $4 \times 4$- Matrizen werden durch die Flatten-Funktion $T_f$ aus Definition \ref{def:flatten} in einen Vektor $f \in \RR^{12 \cdot 4 \cdot 4}$ umgewandelt. Dies sei durch
\begin{equation*}
    \label{eq:f_forw}
    f=T_f(P^2)
\end{equation*}
beschrieben. 
Die Umkehrung wird mit
\begin{equation*}
    \label{eq_f_ford_inv}
    P^2=T_f^{-1}(f).
\end{equation*}
bezeichnet.
Die Ausgabe des FFN ergibt sich schließlich zu
\begin{equation*}
    \label{eq:y_forw}
    y=\psi(W^T f +b).
\end{equation*}
und der mittlere quadratische Fehler lautet
\begin{equation*}
    \label{eq:E_forw}
    E=\frac{1}{2} \sum_{k=1}^{10} \left(y(k)-t(k)\right)^2,
\end{equation*}
wobei $t=t(X,c)$ der Zielvektor des Datenpaars $(X,c)$ ist.

\subsection*{Backpropagation}

Zuerst werden die Gradienten bezüglich der Gewichtsmatrix und Biasvektor der Neuronenschicht des FNN berechnet. Es gilt
\begin{align*}
        \Delta W(j,k) &=  \frac{\partial E}{\partial W(j,k)}\\
                       &=  \frac{\partial L}{\partial y(k)} \cdot \frac{\partial y(k)}{\partial W(j,k)}\\
                       &= (y(k)-t(k)) \cdot \frac{\partial}{\partial W(j,k)} \psi \left(\sum_{j=1}^{192} W(j,k) f(j)+b(k)\right)\\
                       &= (y(k)-t(k)) \cdot y(k)(1-y(k)) \cdot f(j).
\end{align*}
Mit $\delta^{\text{FFN}}(k)=(y(k)-t(k)) \cdot y(k)(1-y(k))$ für $1 \leq k \leq 10$ lässt sich $\Delta W$ als dyadisches Produkt  
\begin{align*}
    \Delta W(j,k)&= \left(\delta^{\text{FFN}}(k) \cdot f(j) \right) \\
    \Rightarrow  \Delta W&=\left(f \otimes (\delta^{\text{FFN}})^T\right)   
\end{align*}
darstellen. Analog gilt
\begin{equation*}
\Delta b= \delta^{\text{FFN}}.
\end{equation*}
Um $ \Delta K^2_{q,p}$ zu bestimmen, ist zunächst der lokale Fehler der darüberliegenden Neuronenschicht zu ermitteln. Der lokale Fehler $\delta^f \in \RR^{192}$ wird mithilfe des lokalen Fehlers des FNN berechnet und lautet
\begin{align}
    \label{eq:delta_f}
    \begin{split}
    \delta^f(j) &=  \frac{\partial E}{\partial f} \\
                &=  \sum_{k=1}^{10} \frac{\partial E}{\partial y(k)} \cdot \frac{\partial y(k)}{\partial f(j)} \\
                &=  (y(k)-t(k)) \cdot \frac{\partial}{\partial f(j)} \psi \left(\sum_{j=1}^{192} W(k,j) f(j) +b(k)\right) \\
                &=-  \sum_{k=1}^{10} (y(k)-t(k)) \cdot y(k)(1-y(k)) \cdot W(k,j) \\
                &=\sum_{k=1}^{10} \delta^{\text{FFN}}(k) \cdot W(k,j) \\
                \Rightarrow \delta^f &= W \delta^{\text{FFN}}.
    \end{split}            
\end{align}
In der Pooling-Schicht $P^2$ sind keine Gradienten zu bestimmen, da diese nicht von den Modellparametern abhängt. Mit
\begin{equation*}
    \delta^{P^2}=T_f^{-1}(\delta^f) \in \RR^{4 \times 4 \times 12}
\end{equation*}
folgt mit der Mittelwert-Funktion $T$ und $\nabla T=\frac{1}{4} \mathbf{1}$ das Upsampling
\begin{align*}
    \delta^{C^2}_q(i,j)&= \delta^{P^2}_q\left( \lceil i/2 \rceil, \lceil j/2 \rceil\right) \cdot \frac{1}{4}, \; \; 1 \leq i,j \leq, 1 \leq q \leq 12.
\end{align*}
Es gilt $\delta^{C^2} \in \RR^{12 \times 8 \times 8}$. Nun kann $\Delta K^2_{q,p}$ an einer Stelle $(u,v)$ als
\begin{align*}
    \Delta K^2_{q,p}(u,v) &=  \frac{\partial E}{\partial K^2_{q,p}(u,v)} \\
                          &=   \sum_{i=1}^8 \sum_{j=1}^8 \frac{\partial E}{\partial C^2_q(i,j)} \cdot \frac{\partial C^2_q(i,j)}{\partial K_{q,p}^2(u,v)} \\
                          &=  \sum_{i=1}^8 \sum_{j=1}^8 \delta_q^{C^2}(i,j) \cdot \frac{\partial}{ K^2_{q,p}(u,v)} \psi \left( \sum_{p=1}^6 \sum_{u=0}^4 \sum_{v=0}^4 P^1_p(i+u,j+v) \cdot K_{q,p}^2(u,v) +b_q^2\right) \\
                          &= - \sum_{i=1}^8 \sum_{j=1}^8 \delta_q^{C^2}(i,j) \cdot C_q^2(i,j)\left(1-C_q^2(i,j)\right) \cdot P^1_p(i+u,j+v)
\end{align*}
für $1 \leq q \leq 12, 1 \leq p \leq 6$ berechnet werden.
Bezeichne
\begin{align*}
    \delta^{C^2}_{q,\psi}(i,j):&= \delta^{C^2}_q(i,j) \cdot C_q^2(i,j)\left(1- C_q^2(i,j)\right)\\
    &= \delta^{C^2}_q(i,j) \cdot \psi'(C^2_q(i,j)), \; \; 1 \leq i,j \leq 8, 1 \leq q \leq 12
\end{align*} den lokalen Fehler $\delta^{C^2}_{q,\psi}$ der Faltungsschicht $C^2$. Damit ergibt sich wegen der Ableitung der logistischen Funktion
\begin{equation*}
    C_{q,\psi}^2(i,j)=\sum_{p=1}^6 \sum_{u=-2}^2 \sum_{v=-2}^2 P^1_p(i+u,j+v) \cdot K^2_{q,p}(u,v) +b_q^2.
\end{equation*}
%und
%\begin{equation*}
%    P^1_{p,rot180}(u-i,v-j):=P^1_p(i-u,j-v).
%\end{equation*}
Es gilt
\begin{align}
    \label{eq:conv_in_train}
    \begin{split}
    \Delta K^2_{q,p}(u,v) &=  \sum_{i=1}^8 \sum_{j=1}^8 P^1_{p}(u+i,v+j) \cdot \delta^{C^2}_{q,\psi}(i,j) \\
     \Rightarrow \Delta K_{q,p}^2 &=  P^1_{p} \ast \delta^{C^2}_{q,\psi} , \; \; 1 \leq q \leq 12, 1 \leq p \leq 6.
    \end{split}
\end{align}
Dies ist besonders für die effiziente Implementierung der Rückwärtsrechnung nützlich, da Abstiegsrichtungen wieder mithilfe von Faltungsoperationen ausgedrückt werden können. Es muss also besonders die Faltung performant implementiert werden, da sie sowohl bei der Vorwärtsrechnung als auch Rückwärtsrechnung genutzt wird. Analog ergibt sich 
\begin{equation*}
    \Delta b_q^2=  \sum_{i=1}^8 \sum_{j=1}^8 \delta_{q,\psi}^{C^2}(i,j), \; \; 1 \leq q \leq 12.
\end{equation*}
Um $ \Delta K^1_{p,1}$ zu bestimmen, ist zunächst der lokale Fehler der darüberliegenden Pooling-Schicht zu ermitteln. Dieser lässt sich wieder mit dem lokalen Fehler $\delta_{q,\psi}^{C^2}$ der Faltungsschicht $C^2$ ausdrücken. Es gilt 
\begin{align*}
    \delta^{P^1}_{p}(i,j) &= \frac{\partial E}{\partial P^1_{p}(i,j)} \\
    &= \sum_{q=1}^{12} \sum_{u=-2}^2 \sum_{v=-2}^2 \frac{\partial E}{\partial C^2_{q,\psi}(i-u,j-v)} \cdot \frac{\partial C^2_{q,\psi}(i-u,j-v)}{\partial P_{p}^1(i,j)} \\
    &= \sum_{q=1}^{12} \sum_{u=-2}^2 \sum_{v=-2}^2 \delta^{C^2}_{q,\psi}(i-u,j-v) \cdot \frac{\partial}{\partial P_p^1(i,j)} \left( \sum_{p=1}^6 \sum_{u=-2}^2 \sum_{v=-2}^2 P^1_p(i,j) \cdot K^2_{q,p}(u,v) +b_q^2\right) \\
    &= \sum_{q=1}^{12} \sum_{u=-2}^2 \sum_{v=-2}^2 \delta_{q,\psi}^{C^2}(i-u,j-v) \cdot K^2_{q,p}(u,v), \; \; 1 \leq i,j \leq 12
\end{align*}
für alle 1 $\leq p \leq 6$. Mit $K^2_{q,p,rot180}(-u,-v):=K^2_{q,p}(u,v)$ ergibt sich
\begin{align*}
    \delta^{P^1}_{p}(i,j)&=\sum_{q=1}^{12} \sum_{u=-2}^2 \sum_{v=-2}^2 \delta_{q,\psi}^{C^2}(i+(-u),j+(-v)) \cdot K^2_{q,p,rot180}(-u,-v) \\
    \Rightarrow \delta^{P^1}_{p} &= \sum_{q=1}^{12} \delta_{q, \psi}^{C^2} \ast K_{q,p,rot180}^2, \; \; 1 \leq p \leq 6.
    \end{align*}
Es gilt  $\delta^{P^1} \in \RR^{6 \times 12 \times 12}$. Mit dem Upsampling
\begin{equation*}
    \delta^{C^1}_p(i,j)= \delta^{P^1}_p \left( \lceil i/2 \rceil, \lceil j/2 \rceil \right) \cdot \frac{1}{4}, \; \; 1 \leq i,j \leq 24 
\end{equation*}
für $1 \leq p \leq 6$ kann nun der Gradient $\Delta K^1_{p,1}$ an einer Stelle $(u,v)$ mit
\begin{align*}
    \Delta K^1_{p,1}(u,v) &= \, \frac{\partial E}{\partial K^1_{p,1}(u,v)} \\
    &= \, \sum_{i=1}^{24} \sum_{j=1}^{24} \frac{\partial E}{\partial C^1_p(i,j)} \cdot \frac{\partial C^1_p(i,j)}{\partial K_{p,1}^1(u,v)} \\
    &=  \, \sum_{i=1}^{24} \sum_{j=1}^{24} \delta_p^{C^1}(i,j) \cdot \frac{\partial}{ K^1_{p,1}(u,v)} \psi \left(\sum_{u=-2}^2 \sum_{v=-2}^2 X(i+u,j+v) \cdot K^1_{p,1}(u,v) +b_p^1\right) \\
    &=  \, \sum_{i=1}^{24} \sum_{j=1}^{24} \delta_p^{C^1}(i,j) \cdot C_p^1(i,j)\left(1-C_p^1(i,j)\right) \cdot X(i+u,j+v).
\end{align*}
für $1 \leq p \leq 6$ berechnet werden. Sei wieder
\begin{align*}
    \delta^{C^1}_{p,\psi}(i,j):&= \delta^{C^1}_p(i,j) \cdot C_p^1(i,j)\left(1- C_p^1(i,j)\right)\\
    &= \delta^{C^1}_p(i,j) \cdot \psi'(C^1_p(i,j)), \; \; 1 \leq i,j \leq 8, 1 \leq p \leq 6.
\end{align*}
Dann gilt schließlich
\begin{align*}
    \Delta K^1_{p,1}(u,v) &= \sum_{i=1}^{24} \sum_{j=1}^{24} X(u+i,v+j) \cdot \delta^{C^1}_{p,\psi}(i,j) \\
     \Rightarrow \Delta K_{p,1}^1 &=  X \ast \delta^{C^1}_{p,\psi} , \; \; 1 \leq p \leq 6
\end{align*} und 
\begin{align*}
    \Delta b_p^1 = \sum_{i=1}^{24} \sum_{j=1}^{24} \delta_{p, \psi}^{C^1}(i,j), \; \; 1 \leq p \leq 6.
\end{align*}
Die Aktualisierung der Parameter mit der Lernrate $\lambda \in \RR$ efolgt durch
\begin{alignat*}{3}
    K^{1}_{p,1}&=K^{1}_{p,1} &&+ \lambda \Delta K^1_{p,1} \\
    b^{1}_{p}&=b^{1}_{p} &&+ \lambda \Delta b^1_{p} \\
    K^{2}_{q,p}&=K^{2}_{q,p} &&+ \lambda \Delta K^2_{q,p} \\
    b^{2}_{q}&=b^{2}_{q} &&+ \lambda \Delta b^2_{q} \\
    W&=W &&+ \lambda \Delta W \\
    b&=b&&+ \lambda \Delta b.
\end{alignat*}
Diese Schritte sind solange zu wiederholen, bis die gewünschte Erkennungsrate bzw. Generalisierungsrate erreicht wird. Für die effiziente Implementierung des Trainingsprozesses sind die Darstellungen als Matrixvektorprodukte, z.B. in Gleichung (\ref{eq:delta_f}), beziehungsweise Faltungsoperationen, vgl. Gleichung (\ref{eq:conv_in_train}), zu nutzen. Diese Operationen sind performant umzusetzen.

In diesem Kapitel wird die Backpropagation bei gefalteten neuronalen Netzen beschrieben. Bei der Darstellung des Trainingsprozesses wird darauf Wert gelegt, die Algorithmen möglichst allgemein zu beschreiben, um
die meisten verwendeten Architekturen abzudecken. Es können also beliebig viele Faltungsschichten, Pooling-Schichten und Neuronenschichten untereinander verknüpft werden. Im folgenden Kapitel \ref{kap:CNN_in_SQL} wird ausgehend von optimierten Modellparameter eines CNN-Modells die datenbankgestützte Umsetzung in SQL diskutiert. Die Problemstellungen \ref{prob:conv_in_sql} und \ref{prob:ffCCN} werden bearbeitet. 
%Im Folgenden werden konkrete Beispiele für verschiedene zweidimensionale Faltungen, welche in dieser Arbeit im Fokus stehen, gegeben. Dabei sind die Eingabe $X \in \RR^{h \times w}$ und der Filter $K \in \RR^{k_h \times k_w}$ immer als Matrizen zu verstehen. Das Ergebnis der Faltung $S= X \ast K$ wird als Merkmalskarte bezeichnet. Es sei angemerkt, dass oft $k_h=k_w$ sowie $k_h$ ungerade gewählt wird, z.B. $k_h=3$ oder $k_h=5$. Die Größe der Merkmalskarte wird durch die Parameter
%\begin{itemize}
%    \item $h,w$: Die Höhe und Breite der Eingabe,
%    \item $k_h, k_w$: Die Abmessungen des Filters,
%   \item $s_h, s_w$: Die Wahl der strides, 
%   \item $p_h, p_w$: Die Größe des zero paddings
%\end{itemize}
%beeinflusst. Mit zero padding ist gemeint, dass künstliche Nullen um Randpixel der Eingabe $X$ eingefügt werden, damit die Berechnung mit dem Filter um jene Pixel gelingt. Ein Beispiel für das Verwenden von zero padding wird in Abbildung \ref{abb_simplematrixconv_padding} gezeigt. In Abbildung \ref{abb_simplematrixconv} ist die Berechnung einer einfachen zweidimensionalen Matrixfaltung dargestellt. Ein vorher festgelegter Filter (grau) bewegt sich über die Eingabe (blau) und berechnet jeweils die Einträge der Ausgabe(grün). 

%\begin{figure}[h]
    %\includegraphics[width=0.8\textwidth]{pics/abb_simpleconv}
    %\centering
    %\caption{Es wird die Merkmalskarte $S \in \RR^{3 \times 3}$ mit den Parametern ${h,w=5}, k_h=k_w=3, s_h=s_w=1$ und $p_h=p_w=1.$}
    %%\label{abb_simplematrixconv}
%\end{figure}

%\begin{figure}[h]
   % \includegraphics[width=0.8\textwidth]{pics/abb_simplecov_padding}
   % \centering
   % \caption{Es wird die Merkmalskarte $S \in \RR^{3 \times 3}$ mit den Parametern ${h,w=5}, k_h=k_w=3, s_h=s_w=2$ und $p_h=p_w=1$ berechnet.}
    %\label{abb_simplematrixconv_padding}
%\end{figure}



%\section{Motivation der Faltung}
%Weiter machen! optionaler Abschnitt falls noch Zeit ist(wenn nicht im Einleitungstext des kapitels entfernen!!)
%Sie nutzt wichtige Konzepte zur Optimierung von Machine-Learning-Verfahren wie spärliche Konnektivität (engl. \textit{sparse connectivity}), \textit{Parameter Sharing} und \textit{äquivariante Repräsentation}, vgl. \cite{goodfellow}. Spärliche Konnektivität bedeutet, dass Neuronen auf einer Schicht $\mathcal{s}_{l+1}$ nur durch wenige Neuronen der Schicht $\mathcal{S}_l$ beeinflusst wird. Dies ist bei CNNs typisch, da meist die verwendeten Filter viel kleiner als die Eingabe ist. Noch mehr erklären + Abbildung

%Mit Parameter Sharing ist die Nutzung von gleichen Parametern für mehrere Funktionen im neuronalen Netz gemeint. In herkömmlichen Feed-Forward-Netzen wird jedes Element der Gewichtsmatrizen für die Berechnung der Aktivierungen der jeweiligen Schichten verwendet. Anschließend werden diese Gewichte dann nicht mehr gebraucht. Im Zusammenhang von CNNs bedeutet Parameter Sharing während der Faltungsoperation, dass nur eine bestimmte Menge von Parametern erlernt werden müssen
%Noch mehr erklären + Abbildung

