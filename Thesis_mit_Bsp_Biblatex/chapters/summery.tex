\chapter{Zusammenfassung und Ausblick}
\label{kap:sum}

In dieser Arbeit wird gezeigt, wie die Mustererkennung in neuronalen Netzen und in gefalteten neuronalen Netzen datenbankgestützt implementiert werden kann. Dazu wird die Vorwärtsrechnung der jeweiligen Modelle in eine Folge von SQL-Anfragen übersetzt. Für die datenbankgestützte Umsetzung der Faltungsoperation werden in dieser Arbeit drei Ansätze vorgestellt, die anschließend hinsichtlich ihrer Einsetzbarkeit in Anwendungen diskutiert werden. Es stellt sich als vorteilhaft heraus, Basisoperationen der linearen Algebra mit dem SQL-Kern zu vereinen, um zufriedenstellende Resultate hinsichtlich der Problemstellung \ref{prob:conv_in_sql} zu erhalten. Daher wurden in Kapitel \ref{kap:fund} Methoden und Implementierungen vorgestellt, um Objekte der linearen Algebra als Relationen darzustellen und damit verbundene Matrixvektoroperationen, insbesondere die dünnbesetzte Matrixvektormultiplikation, datenbankgestützt umzusetzen.  

In der Erkennungsphase werden trainierte Modelle genutzt, um erkannte Muster in einer Zieldatenmenge effizient ableiten zu können. Die Trainingsphase neuronaler Netze wird in dieser Arbeit mithilfe der Backpropagation realisiert. Dieser Algorithmus wird für vorwärtgerichtete neuronale Netze im Kapitel \ref{kap:NN} und für gefaltete neuronale Netze im Kapitel \ref{kap:CNN} verallgemeinert. In der Literatur wird die Rückwärtsrechnung bei CNN meist nur für spezielle Modelle erläutert beziehungsweise auf die Backpropagation bei neuronalen Netzen verwiesen. In dieser Arbeit wird sowohl die Backpropagation bei FFN sowie CNN algorithmisch beschrieben, um dem Leser die Möglichkeit zu geben, selbst eigene Modelle zu trainieren und zu verwenden.
Bei der Darstellung der Trainingsprozesse wird darauf Wert gelegt, die Algorithmen möglichst allgemein zu beschreiben, um die meisten verwendeten Architekturen abzudecken. Auch hier ist zu beobachten, dass der Trainingsprozess mit Matrixvektoroperationen bzw. Faltungsoperationen umgesetzt werden kann. Die performante Implementierung dieser Funktionen ist im Hinblick auf Problem \ref{prop:train} zu gewährleisten. 

In dieser Arbeit wird das Modell \ref{modell} genutzt, um handgeschriebene Ziffern aus dem MNIST-Datensatz zu klassifizieren. Die Vorwärts- und Rückwärtsrechnung dieses Modells ist im Abschnitt \ref{abs:model_mnist} ausführlich beschrieben. Zusammen mit den Resultaten aus Kapitel \ref{kap:CNN_in_SQL} gelingt in dieser Arbeit die datenbankgestützte Vorwärtsrechnung für dieses trainierte Modell. Damit wird Problemstellung \ref{prob:ffCCN} gelöst. Schließlich soll noch ein Ausblick auf weitere Forschungsthemen gegeben werden.

\section*{Datenbankgestützte Trainingsphase}
In den Abschnitten \ref{abs:task_training} und \ref{abs:CNN_train} wird der Online-Backpropagationsalgorithmus erläutert. Dabei müssen bestimmte Abstiegsrichtungen bestimmt werden. Die effiziente Berechnung dieser Abstiegsrichtungen gehört wohl zu den schwersten Aufgaben des Maschinellen Lernens. Die datenbankgestützte Umsetzung der Trainingsphase in (parallelen) Datenbankmanagementsystemen könnte dabei Abhilfe schaffen und ist zu untersuchen. 

\section*{Schnelle Fourier-Transformation in SQL}
Im Abschnitt \ref{abs:conv_in_sql} wird die Faltung mithilfe der diskreten Fourier-Transformation beleuchtet und eine datenbankgestützte Implementierung in SQL vorgestellt. Dabei wird die Transformation als Matrixmatrixprodukt $F X F^T$ berechnet, was insbesondere für große Matrizen zu langsamen Laufzeiten führt. Wird zur Berechnung der DFT die sogenannte schnelle Fourier-Transformation genutzt, können die Zeitkosten von $\mathcal{O}(n^2)$ auf $\mathcal{O}(n \log n)$ vermindert werden. Verschiedene Möglichkeiten, die eindimensionale FFT in SQL zu implementieren, werden in Marten et. al.\cite{DBLP:conf/adbis/Marten0019} präsentiert. Die Verallgemeinerung für die zweidimensionale FFT und deren Nutzung zur Berechnung von Faltungen ist zu diskutieren.

\section*{Darstellung der Kerne}
Bei CNN werden trainierbare Kerne genutzt, mit denen die Faltungen durchgeführt werden. Im Abschnitt \ref{abs:conv_in_sql} wird ein Ansatz erläutert, bei dem Kerne in Blockmatrizen überführt werden. Diese Matrizen weisen eine gewisse Bandstruktur auf, welche in Relationen überführt werden kann. So sollte es möglich sein, hinsichtlich des Zeit- und Speicheraufwands bessere Resultate zu erzielen. Darüber hinaus sind bestimmte Kerne seperabel \cite{DBLP:journals/tcas/BaiZH18}. Auch diese Eigenschaft ist in zukünftiger Forschung zu diskutieren.

\section*{Systemanpassung}
Während der gesamten Arbeit wurden die Laufzeiten von SQL-Anfragen im Datenbankmanagementsystem PostgreSQL gemessen. Die verwendete Parameterkonfiguration von PostgreSQL ist im Abschnitt \ref{abs:conv_in_sql} angegeben. Eine Rekonfiguration kann zu besseren Laufzeiten für die in Kapitel \ref{kap:CNN_in_SQL} vorgeführten SQL-Anfagen führen und ist untersuchen. Schließlich sollten Faltungsoperationen somit schneller berechnet werden können.