\chapter{Zusammenfassung und Ausblick}
\label{kap:sum}

In dieser Arbeit wird gezeigt, wie die Mustererkennung in neuronalen Netzen und in gefalteten neuronalen Netzen datenbankgestützt implementiert werden kann. Dazu wird die Vorwärtsrechnung der jeweiligen Modelle in eine Folge von SQL-Anfragen übersetzt. Es stellt sich als vorteilhaft heraus, Basisoperationen der linearen Algebra mit dem SQL-Kern zu vereinen, um zufriedenstellende Resultate hinsichtlich der Problemstellung \ref{prob:conv_in_sql} zu erhalten. Daher wurden in Kapitel \ref{kap:fund} Methoden und Implementierungen vorgestellt, um Objekte der linearen Algebra als Relationen darzustellen und damit verbundene Matrixvektoroperationen, insbesondere die dünnbesetzte Matrixvektormultiplikation, datenbankgestützt umzusetzen.

In der Erkennungsphase werden trainierte Modelle genutzt, um erkannte Muster in einer Zieldatenmenge effizient ableiten zu können. Die Trainingsphase neuronaler Netze wird in dieser Arbeit mithilfe der Backpropagation realisiert. Dieser Algorithmus wird für vorwärtgerichtete neuronale Netze im Kapitel \ref{kap:NN} und für gefaltete neuronale Netze im Kapitel \ref{kap:CNN} beleuchtet. Auch hier ist zu beobachten, dass der Trainingsprozess mit Matrixvektoroperationen bzw. Faltungsoperationen umgesetzt werden kann. Die performante Implementierung dieser Funktionen ist im Hinblick auf Problem \ref{prop:train} zu gewährleisten. 

In dieser Arbeit wird das Modell \ref{modell} genutzt, um handgeschriebene Ziffern aus dem MNIST-Datensatz zu klassifizieren. Die Vorwärts- und Rückwärtsrechnung dieses Modells ist im Abschnitt \ref{abs:model_mnist} ausführlich beschrieben. Zusammen mit den Resultaten aus Kapitel \ref{kap:CNN_in_SQL} gelingt die datenbankgestützte Vorwärtsrechnung für dieses trainierte Modell. Damit wird Problemstellung \ref{prob:ffCCN} gelöst. Schließlich soll noch ein Ausblick auf weitere Forschungsthemen gegeben werden.

\section*{Trainingsphase in SQL}
In den Abschnitten \ref{abs:task_training} und \ref{abs:CNN_train} wird der Online-Backpropagationsalgorithmus erläutert. Dabei müssen bestimmte Abstiegsrichtungen bestimmt werden. Die effiziente Berechnung dieser Abstiegsrichtungen gehört wohl zu den schwersten Aufgaben des Maschinellen Lernens. Die datenbankgestützte Umsetzung der Trainingsphase in (parallelen) Datenbankmanagementsystemen könnte dabei Abhilfe schaffen und ist zu untersuchen. 

\section*{FFT in SQL}
Im Abschnitt \ref{abs:conv_in_sql} wird die Faltung mithilfe der diskreten Fourier-Transformation beleuchtet und eine datenbankgestützte Implementierung in SQL vorgestellt. Dabei wird die Transformation als Matrixmatrixprodukt $F X F^T$ berechnet, was insbesondere für große Matrizen zu langsamen Laufzeiten führt. Wird zur Berechnung der DFT die sogenannte schnelle Fourier-Transformation genutzt, können die Zeitkosten von $\mathcal{O}(n^2)$ auf $\mathcal{O}(n \log n)$ vermindert werden. Verschiedene Möglichkeiten, die eindimensionale FFT in SQL zu implementieren, werden in Marten et. al.\cite{DBLP:conf/adbis/Marten0019} präsentiert. Die Verallgemeinerung für die zweidimensionale FFT und deren Nutzung zur Berechnung von Faltungen ist zu diskutieren.

\section*{Darstellung der Kerne}
Bei CNN werden trainierbare Kerne genutzt, mit denen die Faltungen durchgeführt werden. Im Abschnitt \ref{abs:conv_in_sql} wird ein Ansatz erläutert, bei dem Kerne in Blockmatrizen überführt werden. Diese Matrizen weisen eine gewisse Bandstruktur auf, welche in Relationen zu überführen ist. So sollte es möglich sein, hinsichtlich des Zeit- und Speicheraufwands bessere Resultate zu erzielen. Darüber hinaus sind bestimmte Kerne seperabel \cite{}. Auch diese Eigenschaft ist in zukünftiger Forschung zu diskutieren.

\section*{Systemanpassung}
Während der gesamten Arbeit wurden die Laufzeiten von SQL-Anfragen im Datenbankmanagementsystem PostgreSQL gemessen. Außerdem wurde die Testanfragen auf einem Heimsystem bearbeitet, welches nicht weiter angepasst wurde