@article{forster1983analysis,
  title={Analysis 1},
  author={Forster, Otto},
  journal={Vieweg, Braunschweig},
  year={1983},
  publisher={Springer}
}

@article{rosenblatt1958perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}

@article{werbos1988generalization,
  title={Generalization of backpropagation with application to a recurrent gas market model},
  author={Werbos, Paul J},
  journal={Neural networks},
  volume={1},
  number={4},
  pages={339--356},
  year={1988},
  publisher={Elsevier}
}

@book{minsky2017perceptrons,
  title={Perceptrons, Reissue of the 1988 Expanded Edition with a new foreword by L{\'e}on Bottou: An Introduction to Computational Geometry},
  author={Minsky, Marvin and Papert, Seymour A},
  year={2017},
  publisher={MIT press}
}

@article{bourlard1990links,
  title={Links between Markov models and multilayer perceptrons},
  author={Bourlard, Herve and Wellekens, Christian J},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={12},
  number={12},
  pages={1167--1178},
  year={1990},
  publisher={IEEE}
}

@inproceedings{bounds1988multilayer,
  title={A multilayer perceptron network for the diagnosis of low back pain.},
  author={Bounds, David G and Lloyd, Paul J and Mathew, Bruce G and Waddell, Gordon},
  booktitle={ICNN},
  volume={2},
  pages={481--489},
  year={1988}
}

@book{MLPbook,
author = {Rumelhart, and E., David and Mcclelland, James and L., James},
year = {1986},
month = {01},
pages = {},
title = {Parallel distributed processing: explorations in the microstructure of cognition. Volume 1. Foundations}
}

@ARTICLE{radialbasis,  
author={Park, J. and Sandberg, I. W.},  
journal={Neural Computation},   
title={Universal Approximation Using Radial-Basis-Function Networks},   
year={1991},  
volume={3},  
number={2},  
pages={246-257},  
doi={10.1162/neco.1991.3.2.246}}

@article{denker1990transforming,
  title={Transforming neural-net output levels to probability distributions},
  author={Denker, John and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={3},
  year={1990}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@book{hassoun1995fundamentals,
  title={Fundamentals of artificial neural networks},
  author={Hassoun, Mohamad H and others},
  year={1995},
  publisher={MIT press}
}

@article{sonoda2017neural,
  title={Neural network with unbounded activation functions is universal approximator},
  author={Sonoda, Sho and Murata, Noboru},
  journal={Applied and Computational Harmonic Analysis},
  volume={43},
  number={2},
  pages={233--268},
  year={2017},
  publisher={Elsevier}
}

@article{schmidt2020nonparametric,
  title={Nonparametric regression using deep neural networks with ReLU activation function},
  author={Schmidt-Hieber, Johannes},
  journal={The Annals of Statistics},
  volume={48},
  number={4},
  pages={1875--1897},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}

@article{li2017convergence,
  title={Convergence analysis of two-layer neural networks with relu activation},
  author={Li, Yuanzhi and Yuan, Yang},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@book{ruschendorf2014mathematische,
  title={Mathematische Statistik},
  author={R{\"u}schendorf, Ludger},
  volume={62},
  year={2014},
  publisher={Springer}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}

@book{nocedal1999numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen J},
  year={1999},
  publisher={Springer}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey and others},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@article{hanin2018neural,
  title={Which neural net architectures give rise to exploding and vanishing gradients?},
  author={Hanin, Boris},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@book{forster2017analysis,
  title={Analysis 2: Differentialrechnung im IRn, gew{\"o}hnliche Differentialgleichungen},
  author={Forster, Otto},
  year={2017},
  publisher={Springer-Verlag}
}

@book{rojas96neural,
  title = {Neural Networks - A Systematic Introduction},
  author = {Rojas, Raul},
  publisher = {Springer-Verlag},
  year = {1996}
}

@article{blum1992training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim L and Rivest, Ronald L},
  journal={Neural Networks},
  volume={5},
  number={1},
  pages={117--127},
  year={1992},
  publisher={Elsevier}
}

#Kapitel 3
----------------------------------------------------

neural networks intro
@book{dayhoff1990neural,
  title={Neural network architectures: an introduction},
  author={Dayhoff, Judith E},
  year={1990},
  publisher={Van Nostrand Reinhold Co.}
}

#anwendung nn pattern recognition
@book{pandya1995pattern,
  title={Pattern recognition with neural networks in C++},
  author={Pandya, Abhijit S and Macy, Robert B},
  year={1995},
  publisher={CRC press}
}

@article{pao1989adaptive,
  title={Adaptive pattern recognition and neural networks},
  author={Pao, Yohhan},
  year={1989},
  publisher={Reading, MA (US); Addison-Wesley Publishing Co., Inc.}

  
}

@article{urbaniak2021quality,
  title={Quality assessment of compressed and resized medical images based on pattern recognition using a convolutional neural network},
  author={Urbaniak, Ilona and Wolter, Marcin},
  journal={Communications in Nonlinear Science and Numerical Simulation},
  volume={95},
  pages={105582},
  year={2021},
  publisher={Elsevier}
}